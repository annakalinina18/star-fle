{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6ieKzoY+fIfCHHi6K1Ogc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import re\n","from tqdm import tqdm\n","\n","tqdm.pandas()\n","\n","OCC_PATH = \"mwe_occurrences_wide_restored_metadata_v4.xlsx\"\n","CORPUS_PATH = \"corpus_with_mwes_by_level.xlsx\"\n","OUT_PATH = \"corpus_projected_with_types.xlsx\"\n","\n","occ = pd.read_excel(OCC_PATH)\n","corpus = pd.read_excel(CORPUS_PATH)\n","\n","# ---------------- helpers ----------------\n","def normalize(s: str) -> str:\n","    if not isinstance(s, str):\n","        return \"\"\n","    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n","\n","def parse_occ(cell):\n","    \"\"\"\n","    Format attendu: [niveau] [source] phrase\n","    Renvoie: (source, phrase) ou None\n","    \"\"\"\n","    if not isinstance(cell, str) or not cell.strip():\n","        return None\n","    parts = cell.split(\"]\")\n","    if len(parts) < 3:\n","        return None\n","    source = parts[1].replace(\"[\", \"\").strip()\n","    sent = \"]\".join(parts[2:]).strip()\n","    return source, sent\n","\n","def ann_type(s):\n","    # type = annotation_resolved avant la première parenthèse\n","    if not isinstance(s, str):\n","        return \"\"\n","    return s.split(\"(\")[0].strip()\n","\n","# ---------------- find columns ----------------\n","# sentence column in corpus\n","sent_col = next(\n","    (c for c in corpus.columns if c.lower() in [\"sentence\", \"sent\", \"phrase\", \"text\", \"texte\"]),\n","    None\n",")\n","if sent_col is None:\n","    raise ValueError(\"Je ne trouve pas la colonne sentence/phrase/text dans le corpus.\")\n","\n","# source column in corpus (optionnel)\n","source_col = next((c for c in corpus.columns if \"source\" in c.lower()), None)\n","\n","# sent_id: keep if exists, else create stable sequential id\n","sent_id_col = next((c for c in corpus.columns if c.lower() in [\"sent_id\", \"sentence_id\", \"id_sent\", \"id\"]), None)\n","if sent_id_col is None:\n","    sent_id_col = \"sent_id\"\n","    corpus.insert(0, sent_id_col, range(len(corpus)))  # 0..N-1, stable\n","\n","# occ columns and required columns in occ file\n","occ_cols = [c for c in occ.columns if re.fullmatch(r\"occ_\\d+\", str(c))]\n","expr_col = next((c for c in occ.columns if c.lower() in [\"expression\", \"expr\", \"mwe\"]), \"expression\")\n","ann_col = next((c for c in occ.columns if c.lower() == \"annotation_resolved\"), None)\n","if ann_col is None:\n","    raise ValueError(\"Je ne trouve pas la colonne annotation_resolved dans le fichier occurrences.\")\n","\n","# ---------------- build corpus index (by normalized sentence) ----------------\n","print(\"Normalisation des phrases du corpus...\")\n","corpus[\"_norm\"] = corpus[sent_col].progress_apply(normalize)\n","\n","print(\"Indexation du corpus...\")\n","index = {}  # norm_sentence -> list of corpus row indices\n","for i, norm in tqdm(enumerate(corpus[\"_norm\"].tolist()), total=len(corpus), desc=\"Indexing corpus\"):\n","    index.setdefault(norm, []).append(i)\n","\n","# ---------------- collect matches from occurrences ----------------\n","print(\"Lecture des occurrences (wide -> matches)...\")\n","# We'll accumulate on corpus row position i (0..N-1)\n","proj = [dict() for _ in range(len(corpus))]  # each is {expr: type}\n","\n","for _, r in tqdm(occ.iterrows(), total=len(occ), desc=\"Scanning expressions\"):\n","    expr = str(r.get(expr_col, \"\")).strip()\n","    typ = ann_type(r.get(ann_col, \"\"))\n","\n","    if not expr:\n","        continue\n","\n","    for c in occ_cols:\n","        parsed = parse_occ(r.get(c, \"\"))\n","        if not parsed:\n","            continue\n","\n","        occ_source, occ_sent = parsed\n","        key = normalize(occ_sent)\n","        if not key:\n","            continue\n","\n","        # 1) try exact sentence match\n","        hit_rows = index.get(key, [])\n","\n","        # 2) (optionnel) если exact sentence не найдено, можно попробовать fallback по source+подстроке,\n","        # но ты просила: \"чаще всего source, если нет — по тексту\".\n","        # Чтобы не ловить ложные совпадения, fallback делаем очень осторожно:\n","        if not hit_rows:\n","            # fallback: искать по подстроке (первые 120 символов) + source если есть\n","            needle = normalize(occ_sent)[:120]\n","            if needle:\n","                if source_col and isinstance(occ_source, str) and occ_source.strip():\n","                    mask = (\n","                        corpus[source_col].astype(str).str.contains(re.escape(occ_source), na=False)\n","                        & corpus[\"_norm\"].str.contains(re.escape(needle), na=False)\n","                    )\n","                else:\n","                    mask = corpus[\"_norm\"].str.contains(re.escape(needle), na=False)\n","                hit_rows = list(corpus.index[mask])\n","\n","        for i in hit_rows:\n","            # store expr -> type (keep first type if multiple)\n","            if expr not in proj[i]:\n","                proj[i][expr] = typ\n","\n","# ---------------- build MWEs column: \"expr (type)\" ----------------\n","def format_expr_type(d: dict) -> str:\n","    if not d:\n","        return \"\"\n","    # stable alphabetical order\n","    items = [f\"{e} ({t})\" if t else f\"{e}\" for e, t in sorted(d.items(), key=lambda x: x[0])]\n","    return \" | \".join(items)\n","\n","mwes_col_name = \"MWEs_projected\"\n","corpus[mwes_col_name] = [format_expr_type(d) for d in proj]\n","\n","# ---------------- place MWEs column right after sent_id ----------------\n","cols = list(corpus.columns)\n","# remove temp _norm\n","cols.remove(\"_norm\")\n","corpus = corpus.drop(columns=[\"_norm\"])\n","\n","cols = list(corpus.columns)\n","# move MWEs_projected right after sent_id_col\n","cols.remove(mwes_col_name)\n","sent_pos = cols.index(sent_id_col)\n","cols.insert(sent_pos + 1, mwes_col_name)\n","corpus = corpus[cols]\n","\n","# ---------------- save without changing order ----------------\n","corpus.to_excel(OUT_PATH, index=False)\n","print(\"Saved:\", OUT_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EA2WYAh3wu9C","executionInfo":{"status":"ok","timestamp":1770383282720,"user_tz":-180,"elapsed":82926,"user":{"displayName":"Anna Kalinina","userId":"16156166223513079181"}},"outputId":"03d44ad1-f2bb-4f28-a83f-b8d5df98e855"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Normalisation des phrases du corpus...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 49255/49255 [00:00<00:00, 115568.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Indexation du corpus...\n"]},{"output_type":"stream","name":"stderr","text":["Indexing corpus: 100%|██████████| 49255/49255 [00:00<00:00, 674268.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Lecture des occurrences (wide -> matches)...\n"]},{"output_type":"stream","name":"stderr","text":["Scanning expressions: 100%|██████████| 1881/1881 [00:46<00:00, 40.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saved: corpus_projected_with_types.xlsx\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","IN_PATH = \"corpus_projected_with_types.xlsx\"\n","OUT_PATH = \"corpus_projected_with_types_sorted.xlsx\"\n","\n","df = pd.read_excel(IN_PATH)\n","\n","# 1) Найти колонку с sentence_id\n","sid_col = None\n","for c in df.columns:\n","    if c.lower() in [\"sentence_id\", \"sent_id\", \"id_sentence\", \"id_sent\"]:\n","        sid_col = c\n","        break\n","if sid_col is None:\n","    raise ValueError(\"Не нашла колонку sentence_id / sent_id.\")\n","\n","#.any_source_file: выбираем колонку документа (если есть)\n","doc_col = None\n","for c in df.columns:\n","    cl = c.lower()\n","    if cl == \"source_file\" or cl == \"source\" or \"source\" in cl or \"document\" in cl:\n","        doc_col = c\n","        break\n","\n","# 2) Сортируем СТАБИЛЬНО (чтобы одинаковые id не перемешались случайно)\n","# Если есть doc_col, сортируем сначала по документу, потом по sentence_id.\n","if doc_col:\n","    df_sorted = df.sort_values([doc_col, sid_col], kind=\"stable\")\n","else:\n","    df_sorted = df.sort_values([sid_col], kind=\"stable\")\n","\n","df_sorted.to_excel(OUT_PATH, index=False)\n","print(\"Saved:\", OUT_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODr9oKR6zsDb","executionInfo":{"status":"ok","timestamp":1770383330649,"user_tz":-180,"elapsed":35992,"user":{"displayName":"Anna Kalinina","userId":"16156166223513079181"}},"outputId":"f0241716-644f-4b73-82da-609b9ce6b727"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: corpus_projected_with_types_sorted.xlsx\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from tqdm import tqdm\n","\n","OCC_IN = \"mwe_occurrences_wide_cleaned_rightmost_level.xlsx\"\n","CORPUS = \"corpus_with_mwes_by_level.xlsx\"\n","OCC_OUT = \"mwe_occurrences_wide_restored_metadata_v3.xlsx\"\n","\n","occ = pd.read_excel(OCC_IN)\n","corpus = pd.read_excel(CORPUS)\n","\n","# --- corpus columns ---\n","sent_col = next(c for c in corpus.columns\n","                if c.lower() in [\"sentence\",\"sent\",\"phrase\",\"text\",\"texte\"])\n","source_col = next((c for c in corpus.columns if \"source\" in c.lower()), None)\n","niveau_col = next((c for c in corpus.columns if \"niveau\" in c.lower()), None)\n","\n","# --- normalize ---\n","def norm(s):\n","    if not isinstance(s, str):\n","        return \"\"\n","    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n","\n","# --- detect [] blocks at start ---\n","two_blocks = re.compile(r\"^\\s*\\[[^\\]]*\\]\\s*\\[[^\\]]*\\]\")\n","one_block = re.compile(r\"^\\s*\\[[^\\]]*\\]\")\n","\n","def strip_one_block(s):\n","    return re.sub(one_block, \"\", s, count=1).strip()\n","\n","# --- index corpus ---\n","corpus[\"_norm\"] = corpus[sent_col].astype(str).map(norm)\n","\n","index = {}\n","for i, k in enumerate(corpus[\"_norm\"]):\n","    index.setdefault(k, []).append(i)\n","\n","occ_cols = [c for c in occ.columns if re.fullmatch(r\"occ_\\d+\", str(c))]\n","\n","new_occ = occ.copy()\n","\n","for r_i, row in tqdm(occ.iterrows(), total=len(occ), desc=\"Restoring\"):\n","\n","    for c in occ_cols:\n","        cell = row.get(c, \"\")\n","\n","        if not isinstance(cell, str) or not cell.strip():\n","            continue\n","\n","        s = cell.strip()\n","\n","        # ✅ already good: has two [] []\n","        if two_blocks.match(s):\n","            continue\n","\n","        # ⚠ only one [] -> strip and search\n","        if one_block.match(s):\n","\n","            quote = strip_one_block(s)\n","            key = norm(quote)\n","\n","            hits = index.get(key, [])\n","\n","            # fallback: substring search\n","            if not hits and key:\n","                needle = key[:120]\n","                mask = corpus[\"_norm\"].str.contains(re.escape(needle), na=False)\n","                hits = list(corpus.index[mask])\n","\n","            if not hits:\n","                continue\n","\n","            i0 = hits[0]\n","            src = str(corpus.loc[i0, source_col]) if source_col else \"\"\n","            lvl = str(corpus.loc[i0, niveau_col]) if niveau_col else \"\"\n","\n","            new_occ.at[r_i, c] = f\"[{lvl}] [{src}] {quote}\"\n","\n","new_occ.drop(columns=\"_norm\", errors=\"ignore\").to_excel(OCC_OUT, index=False)\n","\n","print(\"Saved:\", OCC_OUT)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTTVBuYS6cZw","executionInfo":{"status":"ok","timestamp":1770380745028,"user_tz":-180,"elapsed":23532,"user":{"displayName":"Anna Kalinina","userId":"16156166223513079181"}},"outputId":"96ee8451-3223-4da2-a096-795e5ef1c21b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Restoring: 100%|██████████| 1881/1881 [00:01<00:00, 1037.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saved: mwe_occurrences_wide_restored_metadata_v3.xlsx\n"]}]}]}
