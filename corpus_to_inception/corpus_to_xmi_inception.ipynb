{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPPBWg3jDpupy07bT+EEup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annakalinina18/star-fle/blob/main/corpus_to_xmi_inception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, re, os, zipfile, shutil, time\n",
        "from cassis import TypeSystem, Cas\n",
        "\n",
        "xlsx_path = \"corpus_projected_with_types_sorted (2).xlsx\"\n",
        "df = pd.read_excel(xlsx_path)\n",
        "sent_id_col = \"sent_id\"\n",
        "\n",
        "CLITICS = {\"l'\", \"d'\", \"s'\", \"n'\", \"qu'\", \"j'\", \"t'\", \"m'\", \"c'\"}\n",
        "PUNCT_NO_SPACE_BEFORE = {\".\", \",\", \"?\", \"!\", \";\", \":\", \")\", \"]\", \"}\", \"»\"}\n",
        "PUNCT_NO_SPACE_AFTER = {\"(\", \"[\", \"{\", \"«\"}\n",
        "\n",
        "ALLOWED = [\n",
        "    \"Expression_idiomatique\",\n",
        "    \"Collocation_opaque\",\n",
        "    \"Collocation_transparente\",\n",
        "    \"Expression_libre\",\n",
        "    \"Autre\",\n",
        "]\n",
        "\n",
        "def normalize_label(raw: str) -> str:\n",
        "    raw = re.sub(r\"\\s+\", \" \", raw.strip())\n",
        "    key = raw.lower().replace(\" \", \"_\")\n",
        "    mapping = {\n",
        "        \"expression_idiomatique\": \"Expression_idiomatique\",\n",
        "        \"collocation_opaque\": \"Collocation_opaque\",\n",
        "        \"collocation_transparente\": \"Collocation_transparente\",\n",
        "        \"expression_libre\": \"Expression_libre\",\n",
        "        \"autre\": \"Autre\",\n",
        "    }\n",
        "    return mapping.get(key, \"Autre\")\n",
        "\n",
        "def extract_labels(expr_raw: str):\n",
        "    m = re.search(r\"\\((.*?)\\)\\s*$\", expr_raw.strip())\n",
        "    if not m:\n",
        "        return [\"Autre\"]\n",
        "    inside = re.sub(r\"^\\s*ничья\\s*:\\s*\", \"\", m.group(1).strip(), flags=re.I)\n",
        "    parts = [p.strip() for p in inside.split(\"/\") if p.strip()]\n",
        "    labs = [normalize_label(p) for p in parts] if parts else [\"Autre\"]\n",
        "    out=[]; seen=set()\n",
        "    for l in labs:\n",
        "        if l not in seen and l in ALLOWED:\n",
        "            out.append(l); seen.add(l)\n",
        "    return out or [\"Autre\"]\n",
        "\n",
        "def strip_parens(expr_raw: str) -> str:\n",
        "    return re.split(r\"\\s*\\(\", expr_raw.strip(), maxsplit=1)[0].strip()\n",
        "\n",
        "def clean_expr(expr_raw: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", strip_parens(expr_raw)).lower()\n",
        "\n",
        "def explode_apostrophes(expr: str) -> str:\n",
        "    return re.sub(r\"([a-zàâçéèêëîïôûùüÿñæœ]+')([a-zàâçéèêëîïôûùüÿñæœ]+)\", r\"\\1 \\2\", expr, flags=re.I)\n",
        "\n",
        "def expr_to_pattern_tokens(expr_raw: str):\n",
        "    expr = explode_apostrophes(clean_expr(expr_raw))\n",
        "    return [t for t in expr.split() if t and t not in CLITICS]\n",
        "\n",
        "def parse_items(cell):\n",
        "    if pd.isna(cell) or str(cell).strip() in [\"\", \"_\"]:\n",
        "        return []\n",
        "    parts=[p.strip() for p in str(cell).split(\"|\") if p.strip()]\n",
        "    return [{\"raw\":p, \"labels\":extract_labels(p)} for p in parts]\n",
        "\n",
        "def subseq_match_indices(forms, lemmas, pattern, start_pos=0):\n",
        "    idxs=[]; pos=start_pos\n",
        "    for pat in pattern:\n",
        "        found=None\n",
        "        for i in range(pos, len(forms)):\n",
        "            if forms[i]==pat or lemmas[i]==pat:\n",
        "                found=i; break\n",
        "        if found is None:\n",
        "            return None\n",
        "        idxs.append(found); pos=found+1\n",
        "    return idxs\n",
        "\n",
        "def all_subseq_matches(forms, lemmas, pattern):\n",
        "    matches=[]; start=0\n",
        "    while start < len(forms):\n",
        "        idxs=subseq_match_indices(forms, lemmas, pattern, start)\n",
        "        if idxs is None: break\n",
        "        matches.append(idxs); start=idxs[0]+1\n",
        "    matches.sort(key=lambda x:(x[0], x[-1]-x[0]))\n",
        "    return matches\n",
        "\n",
        "def reconstruct_sentence(tokens):\n",
        "    text_parts=[]; offsets=[]; cur=0; prev=None\n",
        "    for tok in tokens:\n",
        "        if prev is None or tok in PUNCT_NO_SPACE_BEFORE or prev in PUNCT_NO_SPACE_AFTER or tok in {\"'\", \"’\"}:\n",
        "            need_space=False\n",
        "        else:\n",
        "            need_space=True\n",
        "        if need_space:\n",
        "            text_parts.append(\" \"); cur+=1\n",
        "        offsets.append(cur)\n",
        "        text_parts.append(tok); cur+=len(tok); prev=tok\n",
        "    return \"\".join(text_parts), offsets\n",
        "\n",
        "# TypeSystem\n",
        "ts = TypeSystem()\n",
        "Sentence = ts.create_type(\"de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence\", \"uima.tcas.Annotation\")\n",
        "SpanEP = ts.create_type(\"webanno.custom.Span_EP\", \"uima.tcas.Annotation\")\n",
        "RelEP = ts.create_type(\"webanno.custom.Relation_EP\", \"uima.tcas.Annotation\")\n",
        "ts.create_feature(RelEP, \"Governor\", \"webanno.custom.Span_EP\")\n",
        "ts.create_feature(RelEP, \"Dependent\", \"webanno.custom.Span_EP\")\n",
        "ts.create_feature(RelEP, \"label\", \"uima.cas.String\")\n",
        "\n",
        "# document boundaries\n",
        "sent_ids = df[sent_id_col].fillna(-1).astype(int).tolist()\n",
        "doc_starts=[0]\n",
        "for i in range(1,len(sent_ids)):\n",
        "    if sent_ids[i]==0: doc_starts.append(i)\n",
        "doc_starts.append(len(df))\n",
        "doc_ranges=[(doc_starts[i], doc_starts[i+1]) for i in range(len(doc_starts)-1)]\n",
        "\n",
        "out_dir=\"/mnt/data/inception_span_relation_labels_noNV\"\n",
        "if os.path.exists(out_dir): shutil.rmtree(out_dir)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "xmi_paths=[]\n",
        "t0=time.time()\n",
        "\n",
        "for doc_idx,(a,b) in enumerate(doc_ranges, start=1):\n",
        "    doc_df=df.iloc[a:b]\n",
        "    cas=Cas(typesystem=ts)\n",
        "    full_text_parts=[]; sent_spans=[]; rel_records=[]\n",
        "    cursor=0\n",
        "\n",
        "    for _, row in doc_df.iterrows():\n",
        "        tokens=str(row.get(\"ud_tokens\",\"\")).split()\n",
        "        if not tokens: continue\n",
        "        forms=[t.lower() for t in tokens]\n",
        "        lemmas_raw=[t.lower() for t in str(row.get(\"ud_lemmas\",\"\")).split()]\n",
        "        lemmas=(lemmas_raw+[\"_\"]*len(tokens))[:len(tokens)]\n",
        "\n",
        "        sent_text, tok_offsets = reconstruct_sentence(tokens)\n",
        "        sent_begin=cursor\n",
        "        full_text_parts.append(sent_text)\n",
        "        sent_end=sent_begin+len(sent_text)\n",
        "        sent_spans.append((sent_begin,sent_end))\n",
        "\n",
        "        items=parse_items(row.get(\"MWEs_projected\", row.get(\"MWE_projected\",\"\")))\n",
        "        used=[False]*len(tokens)\n",
        "\n",
        "        for item in items:\n",
        "            pat=expr_to_pattern_tokens(item[\"raw\"])\n",
        "            if not pat: continue\n",
        "            matches=all_subseq_matches(forms, lemmas, pat)\n",
        "            chosen=None\n",
        "            for idxs in matches:\n",
        "                if not any(used[i] for i in idxs):\n",
        "                    chosen=idxs; break\n",
        "            if chosen is None: continue\n",
        "            for i in chosen: used[i]=True\n",
        "\n",
        "            span_fs=[]\n",
        "            for i in chosen:\n",
        "                tb=sent_begin+tok_offsets[i]\n",
        "                te=tb+len(tokens[i])\n",
        "                fs=SpanEP(begin=tb,end=te)\n",
        "                cas.add(fs)\n",
        "                span_fs.append(fs)\n",
        "\n",
        "            gov=span_fs[0]\n",
        "            for dep in span_fs[1:]:\n",
        "                for lab in item[\"labels\"]:\n",
        "                    rel_records.append((gov,dep,lab))\n",
        "\n",
        "        full_text_parts.append(\"\\n\")\n",
        "        cursor=sent_end+1\n",
        "\n",
        "    full_text=\"\".join(full_text_parts).rstrip(\"\\n\")\n",
        "    cas.sofa_string=full_text\n",
        "    cas.sofa_mime=\"text/plain\"\n",
        "\n",
        "    for b0,e0 in sent_spans:\n",
        "        if e0<=len(full_text):\n",
        "            cas.add(Sentence(begin=b0,end=e0))\n",
        "\n",
        "    for gov,dep,lab in rel_records:\n",
        "        cas.add(RelEP(begin=gov.begin,end=dep.end,Governor=gov,Dependent=dep,label=lab))\n",
        "\n",
        "    xmi_path=os.path.join(out_dir,f\"doc_{doc_idx:03d}.xmi\")\n",
        "    cas.to_xmi(xmi_path, pretty_print=False)\n",
        "    xmi_paths.append(xmi_path)\n",
        "\n",
        "typesystem_path=os.path.join(out_dir,\"TypeSystem.xml\")\n",
        "with open(typesystem_path,\"wb\") as f:\n",
        "    f.write(ts.to_xml().encode(\"utf-8\"))\n",
        "\n",
        "zip_path=\"/mnt/data/inception_span_relation_labels_noNV.zip\"\n",
        "if os.path.exists(zip_path): os.remove(zip_path)\n",
        "with zipfile.ZipFile(zip_path,\"w\",compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    z.write(typesystem_path, arcname=\"TypeSystem.xml\")\n",
        "    for p in xmi_paths:\n",
        "        z.write(p, arcname=os.path.basename(p))\n",
        "\n",
        "(zip_path, len(xmi_paths), round(time.time()-t0,2), os.path.getsize(zip_path))\n"
      ],
      "metadata": {
        "id": "vgWS6IndQSzQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
