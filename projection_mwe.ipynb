{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvhz3W22qGAEY5Qv/wwvE6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import json\n","import pandas as pd\n","from collections import defaultdict\n","from typing import List, Dict, Optional\n","from tqdm import tqdm\n","\n","POLY_PATH = \"polylex_with_ud_stanza.xlsx\"\n","CORPUS_PATH = \"corpus_sent_level_with_ud_stanza.xlsx\"\n","OUT_PATH = \"corpus_with_projected_mwes.xlsx\"\n","\n","def split_str(s):\n","    if s is None or (isinstance(s, float) and pd.isna(s)):\n","        return []\n","    s = str(s).strip()\n","    if not s:\n","        return []\n","    return s.split()\n","\n","def parse_heads(s):\n","    return [int(x) for x in split_str(s)]\n","\n","def parse_ud_row(row):\n","    tokens = split_str(row.get(\"ud_tokens\", \"\"))\n","    lemmas = split_str(row.get(\"ud_lemmas\", \"\"))\n","    upos   = split_str(row.get(\"ud_upos\", \"\"))\n","    deprel = split_str(row.get(\"ud_deprel\", \"\"))\n","    heads  = parse_heads(row.get(\"ud_heads\", \"\"))\n","\n","    n = max(len(lemmas), len(upos), len(deprel), len(heads))\n","    if len(tokens) != n:\n","        tokens = [\"_\"] * n\n","    if len(lemmas) != n:\n","        lemmas = (lemmas + [\"_\"] * n)[:n]\n","    if len(upos) != n:\n","        upos = (upos + [\"_\"] * n)[:n]\n","    if len(deprel) != n:\n","        deprel = (deprel + [\"_\"] * n)[:n]\n","    if len(heads) != n:\n","        heads = (heads + [0] * n)[:n]\n","\n","    deps = defaultdict(list)\n","    for i in range(1, n + 1):\n","        h = heads[i - 1]\n","        deps[h].append(i)\n","\n","    return {\n","        \"n\": n,\n","        \"tokens\": tokens,\n","        \"lemmas\": lemmas,\n","        \"upos\": upos,\n","        \"deprel\": deprel,\n","        \"heads\": heads,\n","        \"deps\": deps\n","    }\n","\n","def build_pattern(poly_row):\n","    ud = parse_ud_row(poly_row)\n","    n = ud[\"n\"]\n","    if n == 0:\n","        return None\n","\n","    roots = [i for i in range(1, n + 1) if ud[\"heads\"][i - 1] == 0]\n","    if not roots:\n","        roots = [i for i in range(1, n + 1) if ud[\"deprel\"][i - 1] == \"root\"]\n","    if not roots:\n","        roots = [1]\n","\n","    edges = []\n","    for i in range(1, n + 1):\n","        h = ud[\"heads\"][i - 1]\n","        if h != 0:\n","            edges.append((i, h, ud[\"deprel\"][i - 1]))\n","\n","    return {\n","        \"expression\": str(poly_row.get(\"expression\", \"\")),\n","        \"n\": n,\n","        \"lemmas\": ud[\"lemmas\"],\n","        \"upos\": ud[\"upos\"],\n","        \"deprel\": ud[\"deprel\"],\n","        \"heads\": ud[\"heads\"],\n","        \"roots\": roots,\n","        \"edges\": edges,\n","    }\n","\n","def find_matches(pattern, sent_ud, max_matches_per_sent=50):\n","    pn = pattern[\"n\"]\n","    sn = sent_ud[\"n\"]\n","    if pn == 0 or sn == 0 or pn > sn:\n","        return []\n","\n","    cand_by_key = defaultdict(list)\n","    for j in range(1, sn + 1):\n","        key = (sent_ud[\"lemmas\"][j - 1], sent_ud[\"upos\"][j - 1])\n","        cand_by_key[key].append(j)\n","\n","    candidates = {}\n","    for i in range(1, pn + 1):\n","        key = (pattern[\"lemmas\"][i - 1], pattern[\"upos\"][i - 1])\n","        candidates[i] = cand_by_key.get(key, [])\n","        if not candidates[i]:\n","            return []\n","\n","    order = sorted(range(1, pn + 1), key=lambda i: len(candidates[i]))\n","\n","    pat_head = {i: pattern[\"heads\"][i - 1] for i in range(1, pn + 1)}\n","    pat_depr = {i: pattern[\"deprel\"][i - 1] for i in range(1, pn + 1)}\n","\n","    child_constraints = {}\n","    for i in range(1, pn + 1):\n","        h = pat_head[i]\n","        if h != 0:\n","            child_constraints[i] = (h, pat_depr[i])\n","\n","    used_sent_nodes = set()\n","    mapping = {}\n","    matches = []\n","\n","    def ok_partial(i):\n","        if i in child_constraints:\n","            h_pat, rel = child_constraints[i]\n","            if h_pat in mapping:\n","                j_child = mapping[i]\n","                j_head = mapping[h_pat]\n","                if sent_ud[\"heads\"][j_child - 1] != j_head:\n","                    return False\n","                if sent_ud[\"deprel\"][j_child - 1] != rel:\n","                    return False\n","        return True\n","\n","    def backtrack(k):\n","        if len(matches) >= max_matches_per_sent:\n","            return\n","        if k == len(order):\n","            mapped_ids = sorted(mapping.values())\n","            matches.append({\n","                \"expression\": pattern[\"expression\"],\n","                \"token_ids_1based\": mapped_ids,\n","                \"span_minmax_1based\": [mapped_ids[0], mapped_ids[-1]],\n","            })\n","            return\n","\n","        i = order[k]\n","        for j in candidates[i]:\n","            if j in used_sent_nodes:\n","                continue\n","            mapping[i] = j\n","            used_sent_nodes.add(j)\n","            if ok_partial(i):\n","                backtrack(k + 1)\n","            used_sent_nodes.remove(j)\n","            del mapping[i]\n","\n","    backtrack(0)\n","\n","    seen = set()\n","    uniq = []\n","    for m in matches:\n","        key = tuple(m[\"token_ids_1based\"])\n","        if key not in seen:\n","            seen.add(key)\n","            uniq.append(m)\n","    return uniq\n","\n","def main():\n","    corpus_df = pd.read_excel(CORPUS_PATH)\n","    poly_df = pd.read_excel(POLY_PATH)\n","\n","    patterns = []\n","    for _, r in tqdm(poly_df.iterrows(), total=len(poly_df), desc=\"Building patterns\"):\n","        pat = build_pattern(r)\n","        if pat and pat[\"n\"] > 0 and pat[\"expression\"]:\n","            patterns.append(pat)\n","\n","    print(f\"Loaded {len(patterns)} patterns from PolyLex.\")\n","\n","    projected_all = []\n","    counts = []\n","\n","    for _, row in tqdm(corpus_df.iterrows(),\n","                       total=len(corpus_df),\n","                       desc=\"Projecting MWEs\"):\n","        sent_ud = parse_ud_row(row)\n","        sent_matches = []\n","\n","        sent_keys = set(zip(sent_ud[\"lemmas\"], sent_ud[\"upos\"]))\n","\n","        for pat in patterns:\n","            pat_keys = set(zip(pat[\"lemmas\"], pat[\"upos\"]))\n","            if not pat_keys.issubset(sent_keys):\n","                continue\n","            ms = find_matches(pat, sent_ud)\n","            if ms:\n","                sent_matches.extend(ms)\n","\n","        projected_all.append(json.dumps(sent_matches, ensure_ascii=False))\n","        counts.append(len(sent_matches))\n","\n","    corpus_df[\"projected_mwes\"] = projected_all\n","    corpus_df[\"projected_mwes_count\"] = counts\n","    corpus_df.to_excel(OUT_PATH, index=False)\n","    print(f\"Saved: {OUT_PATH}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkXQtymK5bZ8","executionInfo":{"status":"ok","timestamp":1770045052311,"user_tz":-180,"elapsed":138300,"user":{"displayName":"Anna Kalinina","userId":"16156166223513079181"}},"outputId":"d4503eb7-ec8e-41d3-a97e-4e3963d982ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Building patterns: 100%|██████████| 2260/2260 [00:00<00:00, 13667.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded 2260 patterns from PolyLex.\n"]},{"output_type":"stream","name":"stderr","text":["Projecting MWEs: 100%|██████████| 49263/49263 [01:40<00:00, 491.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saved: corpus_with_projected_mwes.xlsx\n"]}]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","from collections import defaultdict\n","from typing import Dict, List, Optional\n","from tqdm import tqdm\n","\n","POLY_PATH = \"polylex_with_ud_stanza.xlsx\"\n","CORPUS_PATH = \"corpus_sent_level_with_ud_stanza.xlsx\"\n","OUT_PATH = \"projected_mwes_by_expression.xlsx\"\n","\n","\n","# ----------------------------\n","# Helpers: robust column picking\n","# ----------------------------\n","def pick_first_existing(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n","    cols = list(df.columns)\n","    lower_map = {c.lower(): c for c in cols}\n","    for cand in candidates:\n","        if cand.lower() in lower_map:\n","            return lower_map[cand.lower()]\n","    return None\n","\n","def pick_by_contains(df: pd.DataFrame, substrings: List[str]) -> Optional[str]:\n","    cols = list(df.columns)\n","    for c in cols:\n","        cl = c.lower()\n","        if any(s in cl for s in substrings):\n","            return c\n","    return None\n","\n","def split_str(s) -> List[str]:\n","    if s is None or (isinstance(s, float) and pd.isna(s)):\n","        return []\n","    s = str(s).strip()\n","    if not s:\n","        return []\n","    return s.split()\n","\n","def parse_heads(s) -> List[int]:\n","    return [int(x) for x in split_str(s)]\n","\n","\n","# ----------------------------\n","# UD parsing (expects whitespace-joined sequences in columns)\n","# ----------------------------\n","def parse_ud_row(row: pd.Series,\n","                 col_tokens: str,\n","                 col_lemmas: str,\n","                 col_upos: str,\n","                 col_deprel: str,\n","                 col_heads: str) -> Dict:\n","\n","    tokens = split_str(row.get(col_tokens, \"\")) if col_tokens else []\n","    lemmas = split_str(row.get(col_lemmas, \"\")) if col_lemmas else []\n","    upos   = split_str(row.get(col_upos, \"\")) if col_upos else []\n","    deprel = split_str(row.get(col_deprel, \"\")) if col_deprel else []\n","    heads  = parse_heads(row.get(col_heads, \"\")) if col_heads else []\n","\n","    n = max(len(lemmas), len(upos), len(deprel), len(heads), len(tokens))\n","    if n == 0:\n","        return {\"n\": 0, \"tokens\": [], \"lemmas\": [], \"upos\": [], \"deprel\": [], \"heads\": []}\n","\n","    def pad(lst, fill):\n","        return (lst + [fill] * n)[:n]\n","\n","    tokens = pad(tokens, \"_\")\n","    lemmas = pad(lemmas, \"_\")\n","    upos   = pad(upos, \"_\")\n","    deprel = pad(deprel, \"_\")\n","    heads  = (heads + [0] * n)[:n]\n","\n","    return {\n","        \"n\": n,\n","        \"tokens\": tokens,\n","        \"lemmas\": lemmas,\n","        \"upos\": upos,\n","        \"deprel\": deprel,\n","        \"heads\": heads,  # 1-based heads, 0 = ROOT\n","    }\n","\n","\n","# ----------------------------\n","# Pattern extraction (from PolyLex rows)\n","# ----------------------------\n","def build_pattern(poly_row: pd.Series,\n","                  col_expr: str,\n","                  col_tokens: str,\n","                  col_lemmas: str,\n","                  col_upos: str,\n","                  col_deprel: str,\n","                  col_heads: str) -> Optional[Dict]:\n","\n","    expr = str(poly_row.get(col_expr, \"\")).strip() if col_expr else \"\"\n","    ud = parse_ud_row(poly_row, col_tokens, col_lemmas, col_upos, col_deprel, col_heads)\n","    if not expr or ud[\"n\"] == 0:\n","        return None\n","\n","    # internal constraints: for each pattern node i with head h!=0 -> sentence head must match mapped(h) and deprel must match\n","    pat_head = {i: ud[\"heads\"][i - 1] for i in range(1, ud[\"n\"] + 1)}\n","    pat_depr = {i: ud[\"deprel\"][i - 1] for i in range(1, ud[\"n\"] + 1)}\n","    constraints = {}\n","    for i, h in pat_head.items():\n","        if h != 0:\n","            constraints[i] = (h, pat_depr[i])\n","\n","    return {\n","        \"expression\": expr,\n","        \"n\": ud[\"n\"],\n","        \"lemmas\": ud[\"lemmas\"],\n","        \"upos\": ud[\"upos\"],\n","        \"constraints\": constraints,  # child_i -> (head_i, deprel_of_child)\n","        \"keys\": set(zip(ud[\"lemmas\"], ud[\"upos\"])),  # for quick prefilter\n","    }\n","\n","\n","# ----------------------------\n","# Matching: strict lemma+upos + internal head/deprel arcs\n","# ----------------------------\n","def find_matches(pattern: Dict, sent_ud: Dict, max_matches_per_sent: int = 50) -> List[Dict]:\n","    pn, sn = pattern[\"n\"], sent_ud[\"n\"]\n","    if pn == 0 or sn == 0 or pn > sn:\n","        return []\n","\n","    # sentence index: (lemma, upos) -> positions\n","    cand_by_key = defaultdict(list)\n","    for j in range(1, sn + 1):\n","        cand_by_key[(sent_ud[\"lemmas\"][j - 1], sent_ud[\"upos\"][j - 1])].append(j)\n","\n","    # candidates for each pattern node\n","    candidates = {}\n","    for i in range(1, pn + 1):\n","        key = (pattern[\"lemmas\"][i - 1], pattern[\"upos\"][i - 1])\n","        candidates[i] = cand_by_key.get(key, [])\n","        if not candidates[i]:\n","            return []\n","\n","    order = sorted(range(1, pn + 1), key=lambda i: len(candidates[i]))\n","    constraints = pattern[\"constraints\"]\n","\n","    used = set()\n","    mapping = {}\n","    matches = []\n","\n","    def ok_partial(i: int) -> bool:\n","        # check constraint i -> head\n","        if i in constraints:\n","            h_pat, rel = constraints[i]\n","            if h_pat in mapping:\n","                j_child = mapping[i]\n","                j_head = mapping[h_pat]\n","                if sent_ud[\"heads\"][j_child - 1] != j_head:\n","                    return False\n","                if sent_ud[\"deprel\"][j_child - 1] != rel:\n","                    return False\n","        return True\n","\n","    def backtrack(k: int):\n","        if len(matches) >= max_matches_per_sent:\n","            return\n","        if k == len(order):\n","            ids = sorted(mapping.values())\n","            matches.append({\"token_ids_1based\": ids, \"span_minmax_1based\": [ids[0], ids[-1]]})\n","            return\n","\n","        i = order[k]\n","        for j in candidates[i]:\n","            if j in used:\n","                continue\n","            mapping[i] = j\n","            used.add(j)\n","            if ok_partial(i):\n","                backtrack(k + 1)\n","            used.remove(j)\n","            del mapping[i]\n","\n","    backtrack(0)\n","\n","    # dedup by token set\n","    seen = set()\n","    uniq = []\n","    for m in matches:\n","        key = tuple(m[\"token_ids_1based\"])\n","        if key not in seen:\n","            seen.add(key)\n","            uniq.append(m)\n","    return uniq\n","\n","\n","# ----------------------------\n","# Main: project and aggregate BY EXPRESSION\n","# ----------------------------\n","def main():\n","    corpus_df = pd.read_excel(CORPUS_PATH)\n","    poly_df = pd.read_excel(POLY_PATH)\n","\n","    # ---- Detect UD columns (adjust here if your names differ)\n","    # PolyLex columns\n","    poly_expr_col  = pick_first_existing(poly_df, [\"expression\", \"expr\", \"mwe\", \"polylex\"]) or pick_by_contains(poly_df, [\"express\", \"mwe\"])\n","    poly_tokens_col = pick_first_existing(poly_df, [\"ud_tokens\", \"tokens\"])\n","    poly_lemmas_col = pick_first_existing(poly_df, [\"ud_lemmas\", \"lemmas\", \"ud_lemma\"])\n","    poly_upos_col   = pick_first_existing(poly_df, [\"ud_upos\", \"upos\"])\n","    poly_deprel_col = pick_first_existing(poly_df, [\"ud_deprel\", \"deprel\"])\n","    poly_heads_col  = pick_first_existing(poly_df, [\"ud_heads\", \"heads\", \"ud_head\"])\n","\n","    # Corpus columns\n","    corp_tokens_col = pick_first_existing(corpus_df, [\"ud_tokens\", \"tokens\"])\n","    corp_lemmas_col = pick_first_existing(corpus_df, [\"ud_lemmas\", \"lemmas\", \"ud_lemma\"])\n","    corp_upos_col   = pick_first_existing(corpus_df, [\"ud_upos\", \"upos\"])\n","    corp_deprel_col = pick_first_existing(corpus_df, [\"ud_deprel\", \"deprel\"])\n","    corp_heads_col  = pick_first_existing(corpus_df, [\"ud_heads\", \"heads\", \"ud_head\"])\n","\n","    # Metadata columns in corpus: sentence text, niveau, source\n","    sent_text_col = (\n","        pick_first_existing(corpus_df, [\"sentence\", \"sent\", \"text\", \"phrase\"]) or\n","        pick_by_contains(corpus_df, [\"sentence\", \"sent\", \"phrase\", \"texte\"])\n","    )\n","    niveau_col = pick_first_existing(corpus_df, [\"niveau\", \"level\", \"cecr\", \"cefr\"]) or pick_by_contains(corpus_df, [\"niveau\", \"cecr\", \"cefr\", \"level\"])\n","    source_col = pick_first_existing(corpus_df, [\"source\", \"src\"]) or pick_by_contains(corpus_df, [\"source\", \"src\"])\n","\n","    # Fallback: if sentence text column not found, reconstruct from ud_tokens\n","    if sent_text_col is None and corp_tokens_col is not None:\n","        sent_text_col = \"__reconstructed_text__\"\n","        corpus_df[sent_text_col] = corpus_df[corp_tokens_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(split_str(s)))\n","\n","    # ---- Build patterns\n","    patterns = []\n","    for _, r in tqdm(poly_df.iterrows(), total=len(poly_df), desc=\"Building patterns\"):\n","        pat = build_pattern(r, poly_expr_col, poly_tokens_col, poly_lemmas_col, poly_upos_col, poly_deprel_col, poly_heads_col)\n","        if pat:\n","            patterns.append(pat)\n","\n","    # Pre-store pattern keys for speed\n","    print(f\"Patterns loaded: {len(patterns)}\")\n","\n","    # ---- Aggregate results: expression -> occurrences\n","    agg: Dict[str, List[Dict]] = defaultdict(list)\n","\n","    for sent_idx, row in tqdm(corpus_df.iterrows(), total=len(corpus_df), desc=\"Projecting (aggregate by expression)\"):\n","        sent_ud = parse_ud_row(row, corp_tokens_col, corp_lemmas_col, corp_upos_col, corp_deprel_col, corp_heads_col)\n","        if sent_ud[\"n\"] == 0:\n","            continue\n","\n","        sent_keys = set(zip(sent_ud[\"lemmas\"], sent_ud[\"upos\"]))\n","\n","        sent_text = row.get(sent_text_col, \"\") if sent_text_col else \"\"\n","        niveau = row.get(niveau_col, \"\") if niveau_col else \"\"\n","        source = row.get(source_col, \"\") if source_col else \"\"\n","\n","        for pat in patterns:\n","            # quick prefilter\n","            if not pat[\"keys\"].issubset(sent_keys):\n","                continue\n","\n","            matches = find_matches(pat, sent_ud)\n","            if not matches:\n","                continue\n","\n","            # store one record per match\n","            for m in matches:\n","                agg[pat[\"expression\"]].append({\n","                    \"sent_id\": int(sent_idx),\n","                    \"sentence\": str(sent_text),\n","                    \"niveau\": \"\" if (isinstance(niveau, float) and pd.isna(niveau)) else str(niveau),\n","                    \"source\": \"\" if (isinstance(source, float) and pd.isna(source)) else str(source),\n","                    \"token_ids_1based\": m[\"token_ids_1based\"],\n","                    \"span_minmax_1based\": m[\"span_minmax_1based\"],\n","                })\n","\n","    # ---- Build output table (one row per expression)\n","    rows = []\n","    for expr, occs in agg.items():\n","        rows.append({\n","            \"expression\": expr,\n","            \"occurrences_count\": len(occs),\n","            # full list as JSON\n","            \"occurrences\": json.dumps(occs, ensure_ascii=False),\n","            # quick preview: up to first 5 sentences\n","            \"examples_5\": \" ||| \".join([o[\"sentence\"] for o in occs[:5]]),\n","        })\n","\n","    out_df = pd.DataFrame(rows).sort_values(\"occurrences_count\", ascending=False)\n","    out_df.to_excel(OUT_PATH, index=False)\n","    print(f\"Saved: {OUT_PATH}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"018PKysz7c4l","executionInfo":{"status":"ok","timestamp":1770045488012,"user_tz":-180,"elapsed":42570,"user":{"displayName":"Anna Kalinina","userId":"16156166223513079181"}},"outputId":"40946c4f-9ff3-4ad8-d0e0-6fc391b45035"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Building patterns: 100%|██████████| 2260/2260 [00:00<00:00, 13050.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Patterns loaded: 2260\n"]},{"output_type":"stream","name":"stderr","text":["Projecting (aggregate by expression): 100%|██████████| 49263/49263 [00:22<00:00, 2220.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saved: projected_mwes_by_expression.xlsx\n"]}]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","from tqdm import tqdm\n","\n","IN_PATH = \"projected_mwes_by_expression.xlsx\"\n","OUT_PATH = \"projected_mwes_occurrence_level.xlsx\"\n","\n","df = pd.read_excel(IN_PATH)\n","\n","rows = []\n","\n","for _, r in tqdm(df.iterrows(), total=len(df)):\n","    expr = r.get(\"expression\", \"\")\n","    occ_raw = r.get(\"occurrences\", \"\")\n","\n","    if pd.isna(occ_raw) or not str(occ_raw).strip():\n","        continue\n","\n","    occ_raw = str(occ_raw)\n","\n","    # попытка 1: обычный JSON\n","    try:\n","        occ_list = json.loads(occ_raw)\n","    except Exception:\n","        # попытка 2: вытащить хотя бы объекты вручную\n","        print(\"⚠️ JSON error for expression:\", expr)\n","        continue\n","\n","    for occ in occ_list:\n","        rows.append({\n","            \"expression\": expr,\n","            \"sent_id\": occ.get(\"sent_id\"),\n","            \"sentence\": occ.get(\"sentence\"),\n","            \"niveau\": occ.get(\"niveau\"),\n","            \"source\": occ.get(\"source\"),\n","            \"token_ids_1based\": \" \".join(map(str, occ.get(\"token_ids_1based\", []))),\n","            \"span_minmax_1based\": \" \".join(map(str, occ.get(\"span_minmax_1based\", []))),\n","        })\n","\n","out_df = pd.DataFrame(rows)\n","out_df.to_excel(OUT_PATH, index=False)\n","\n","print(\"Saved:\", OUT_PATH)\n","\n","\n","# (опционально) привести пустые строки/NaN\n","out_df[\"niveau\"] = out_df[\"niveau\"].fillna(\"\")\n","out_df[\"source\"] = out_df[\"source\"].fillna(\"\")\n","\n","out_df.to_excel(OUT_PATH, index=False)\n","print(f\"Saved: {OUT_PATH}\")\n","\n","pivot_niveau = out_df.pivot_table(\n","    index=\"expression\",\n","    columns=\"niveau\",\n","    values=\"sent_id\",\n","    aggfunc=\"count\",\n","    fill_value=0\n",")\n","pivot_niveau.to_excel(\"mwe_by_niveau_pivot.xlsx\")\n","\n","pivot_source = out_df.pivot_table(\n","    index=\"expression\",\n","    columns=\"source\",\n","    values=\"sent_id\",\n","    aggfunc=\"count\",\n","    fill_value=0\n",")\n","pivot_source.to_excel(\"mwe_by_source_pivot.xlsx\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smrx9VOs8xEb","executionInfo":{"status":"ok","timestamp":1770045892418,"user_tz":-180,"elapsed":4044,"user":{"displayName":"Anna Kalinina","userId":"16156166223513079181"}},"outputId":"32a42429-1019-45fe-9fc4-c8b8127d4402"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1758/1758 [00:00<00:00, 10772.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["⚠️ JSON error for expression: être à même de\n","⚠️ JSON error for expression: être d'accord avec\n","⚠️ JSON error for expression: faire ça\n","⚠️ JSON error for expression: faire partie\n","⚠️ JSON error for expression: vouloir bien\n","⚠️ JSON error for expression: famille de quatre enfants\n","⚠️ JSON error for expression: avoir un poil dans la main\n","⚠️ JSON error for expression: avoir le cœur sur la main\n","⚠️ JSON error for expression: ne pas avoir froid aux yeux\n","⚠️ JSON error for expression: avoir la tête sur les épaules\n","⚠️ JSON error for expression: avoir la dent dure\n","⚠️ JSON error for expression: faire la tête\n","⚠️ JSON error for expression: avoir le bras long\n","Saved: projected_mwes_occurrence_level.xlsx\n","Saved: projected_mwes_occurrence_level.xlsx\n"]}]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","from collections import defaultdict\n","from tqdm import tqdm\n","from typing import Optional, List, Dict\n","\n","CORPUS_PATH = \"corpus_sent_level_with_ud_stanza.xlsx\"\n","POLY_PATTERNS_PATH = \"polylex_with_ud_stanza.xlsx\"\n","\n","# Si tu as un fichier polylex_maj distinct, mets-le ici.\n","# Sinon laisse None : le script cherchera resolved_annotation dans POLY_PATTERNS_PATH.\n","POLY_MAJ_PATH = None  # ex: \"/mnt/data/polylex_maj.xlsx\"\n","\n","OUT_WIDE_PATH = \"mwe_occurrences_wide.xlsx\"\n","OUT_PIVOT_PATH = \"mwe_par_niveau_pivot.xlsx\"\n","\n","# Limites pratiques (éviter de casser Excel)\n","MAX_OCC_COLS = 500        # max colonnes occ_1..occ_N par expression\n","MAX_SENT_CHARS = 250      # tronquer la phrase dans occ_k pour rester lisible (et éviter 32767 chars)\n","\n","\n","# ----------------------------\n","# Utils\n","# ----------------------------\n","def split_str(s) -> List[str]:\n","    if s is None or (isinstance(s, float) and pd.isna(s)):\n","        return []\n","    s = str(s).strip()\n","    return s.split() if s else []\n","\n","def parse_heads(s) -> List[int]:\n","    return [int(x) for x in split_str(s)]\n","\n","def pick_first_existing(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n","    lower_map = {c.lower(): c for c in df.columns}\n","    for cand in candidates:\n","        if cand.lower() in lower_map:\n","            return lower_map[cand.lower()]\n","    return None\n","\n","def pick_by_contains(df: pd.DataFrame, substrings: List[str]) -> Optional[str]:\n","    for c in df.columns:\n","        cl = c.lower()\n","        if any(s in cl for s in substrings):\n","            return c\n","    return None\n","\n","def safe_text(x) -> str:\n","    if x is None or (isinstance(x, float) and pd.isna(x)):\n","        return \"\"\n","    return str(x)\n","\n","def trim(s: str, n: int) -> str:\n","    s = safe_text(s)\n","    return (s[:n] + \"…\") if len(s) > n else s\n","\n","\n","# ----------------------------\n","# UD parsing (expects whitespace-joined sequences)\n","# ----------------------------\n","def parse_ud_row(row: pd.Series,\n","                 col_tokens: str,\n","                 col_lemmas: str,\n","                 col_upos: str,\n","                 col_deprel: str,\n","                 col_heads: str) -> Dict:\n","    tokens = split_str(row.get(col_tokens, \"\")) if col_tokens else []\n","    lemmas = split_str(row.get(col_lemmas, \"\")) if col_lemmas else []\n","    upos   = split_str(row.get(col_upos, \"\")) if col_upos else []\n","    deprel = split_str(row.get(col_deprel, \"\")) if col_deprel else []\n","    heads  = parse_heads(row.get(col_heads, \"\")) if col_heads else []\n","\n","    n = max(len(tokens), len(lemmas), len(upos), len(deprel), len(heads))\n","    if n == 0:\n","        return {\"n\": 0, \"tokens\": [], \"lemmas\": [], \"upos\": [], \"deprel\": [], \"heads\": []}\n","\n","    def pad(lst, fill):\n","        return (lst + [fill] * n)[:n]\n","\n","    tokens = pad(tokens, \"_\")\n","    lemmas = pad(lemmas, \"_\")\n","    upos   = pad(upos, \"_\")\n","    deprel = pad(deprel, \"_\")\n","    heads  = (heads + [0] * n)[:n]\n","\n","    return {\"n\": n, \"tokens\": tokens, \"lemmas\": lemmas, \"upos\": upos, \"deprel\": deprel, \"heads\": heads}\n","\n","\n","# ----------------------------\n","# Pattern extraction (strict lemma+upos + internal head/deprel)\n","# ----------------------------\n","def build_pattern(poly_row: pd.Series,\n","                  col_expr: str,\n","                  col_tokens: str,\n","                  col_lemmas: str,\n","                  col_upos: str,\n","                  col_deprel: str,\n","                  col_heads: str) -> Optional[Dict]:\n","    expr = safe_text(poly_row.get(col_expr, \"\")).strip() if col_expr else \"\"\n","    ud = parse_ud_row(poly_row, col_tokens, col_lemmas, col_upos, col_deprel, col_heads)\n","    if not expr or ud[\"n\"] == 0:\n","        return None\n","\n","    # constraints: for each pattern node i with head h != 0, sentence head must match mapped(h), and deprel must match\n","    constraints = {}\n","    for i in range(1, ud[\"n\"] + 1):\n","        h = ud[\"heads\"][i - 1]\n","        rel = ud[\"deprel\"][i - 1]\n","        if h != 0:\n","            constraints[i] = (h, rel)\n","\n","    keys = set(zip(ud[\"lemmas\"], ud[\"upos\"]))\n","\n","    return {\n","        \"expression\": expr,\n","        \"n\": ud[\"n\"],\n","        \"lemmas\": ud[\"lemmas\"],\n","        \"upos\": ud[\"upos\"],\n","        \"constraints\": constraints,\n","        \"keys\": keys\n","    }\n","\n","\n","def find_matches(pattern: Dict, sent_ud: Dict, max_matches_per_sent: int = 50) -> List[List[int]]:\n","    pn, sn = pattern[\"n\"], sent_ud[\"n\"]\n","    if pn == 0 or sn == 0 or pn > sn:\n","        return []\n","\n","    # sentence index: (lemma, upos) -> positions\n","    cand_by_key = defaultdict(list)\n","    for j in range(1, sn + 1):\n","        cand_by_key[(sent_ud[\"lemmas\"][j - 1], sent_ud[\"upos\"][j - 1])].append(j)\n","\n","    # candidates per pattern node\n","    candidates = {}\n","    for i in range(1, pn + 1):\n","        key = (pattern[\"lemmas\"][i - 1], pattern[\"upos\"][i - 1])\n","        cands = cand_by_key.get(key, [])\n","        if not cands:\n","            return []\n","        candidates[i] = cands\n","\n","    order = sorted(range(1, pn + 1), key=lambda i: len(candidates[i]))\n","    constraints = pattern[\"constraints\"]\n","\n","    used = set()\n","    mapping = {}\n","    matches = []\n","\n","    def ok_partial(i: int) -> bool:\n","        if i in constraints:\n","            h_pat, rel = constraints[i]\n","            if h_pat in mapping:\n","                j_child = mapping[i]\n","                j_head = mapping[h_pat]\n","                if sent_ud[\"heads\"][j_child - 1] != j_head:\n","                    return False\n","                if sent_ud[\"deprel\"][j_child - 1] != rel:\n","                    return False\n","        return True\n","\n","    def backtrack(k: int):\n","        if len(matches) >= max_matches_per_sent:\n","            return\n","        if k == len(order):\n","            ids = sorted(mapping.values())\n","            matches.append(ids)\n","            return\n","        i = order[k]\n","        for j in candidates[i]:\n","            if j in used:\n","                continue\n","            mapping[i] = j\n","            used.add(j)\n","            if ok_partial(i):\n","                backtrack(k + 1)\n","            used.remove(j)\n","            del mapping[i]\n","\n","    backtrack(0)\n","\n","    # dedup\n","    seen = set()\n","    uniq = []\n","    for ids in matches:\n","        t = tuple(ids)\n","        if t not in seen:\n","            seen.add(t)\n","            uniq.append(ids)\n","    return uniq\n","\n","\n","# ----------------------------\n","# Main\n","# ----------------------------\n","def main():\n","    corpus_df = pd.read_excel(CORPUS_PATH)\n","    poly_df = pd.read_excel(POLY_PATTERNS_PATH)\n","\n","    # --- Column detection (UD columns)\n","    poly_expr_col  = pick_first_existing(poly_df, [\"expression\", \"expr\", \"mwe\"]) or pick_by_contains(poly_df, [\"express\", \"mwe\"])\n","    poly_tokens_col = pick_first_existing(poly_df, [\"ud_tokens\", \"tokens\"])\n","    poly_lemmas_col = pick_first_existing(poly_df, [\"ud_lemmas\", \"lemmas\", \"ud_lemma\"])\n","    poly_upos_col   = pick_first_existing(poly_df, [\"ud_upos\", \"upos\"])\n","    poly_deprel_col = pick_first_existing(poly_df, [\"ud_deprel\", \"deprel\"])\n","    poly_heads_col  = pick_first_existing(poly_df, [\"ud_heads\", \"heads\", \"ud_head\"])\n","\n","    corp_tokens_col = pick_first_existing(corpus_df, [\"ud_tokens\", \"tokens\"])\n","    corp_lemmas_col = pick_first_existing(corpus_df, [\"ud_lemmas\", \"lemmas\", \"ud_lemma\"])\n","    corp_upos_col   = pick_first_existing(corpus_df, [\"ud_upos\", \"upos\"])\n","    corp_deprel_col = pick_first_existing(corpus_df, [\"ud_deprel\", \"deprel\"])\n","    corp_heads_col  = pick_first_existing(corpus_df, [\"ud_heads\", \"heads\", \"ud_head\"])\n","\n","    # metadata columns in corpus\n","    sent_text_col = (\n","        pick_first_existing(corpus_df, [\"sentence\", \"sent\", \"text\", \"phrase\"]) or\n","        pick_by_contains(corpus_df, [\"sentence\", \"sent\", \"phrase\", \"texte\"])\n","    )\n","    niveau_col = pick_first_existing(corpus_df, [\"niveau\", \"level\", \"cecr\", \"cefr\"]) or pick_by_contains(corpus_df, [\"niveau\", \"cecr\", \"cefr\", \"level\"])\n","    source_col = pick_first_existing(corpus_df, [\"source\", \"src\"]) or pick_by_contains(corpus_df, [\"source\", \"src\"])\n","\n","    if sent_text_col is None and corp_tokens_col is not None:\n","        sent_text_col = \"__reconstructed_text__\"\n","        corpus_df[sent_text_col] = corpus_df[corp_tokens_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(split_str(s)))\n","\n","    # --- resolved_annotation mapping (from polylex_maj if provided, else from patterns file)\n","    if POLY_MAJ_PATH:\n","        poly_maj = pd.read_excel(POLY_MAJ_PATH)\n","    else:\n","        poly_maj = poly_df\n","\n","    ann_expr_col = pick_first_existing(poly_maj, [\"expression\", \"expr\", \"mwe\"]) or pick_by_contains(poly_maj, [\"express\", \"mwe\"])\n","    ann_col = pick_first_existing(poly_maj, [\"annotation_resolved\"]) or pick_by_contains(poly_maj, [\"resolved\", \"annotation\"])\n","\n","    if ann_expr_col is None or ann_col is None:\n","        # fallback: create empty annotation\n","        expr2ann = defaultdict(str)\n","    else:\n","        expr2ann = dict(\n","            (safe_text(r[ann_expr_col]).strip(), safe_text(r[ann_col]).strip())\n","            for _, r in poly_maj[[ann_expr_col, ann_col]].dropna().iterrows()\n","        )\n","\n","    # --- Build patterns\n","    patterns = []\n","    for _, r in tqdm(poly_df.iterrows(), total=len(poly_df), desc=\"Building patterns\"):\n","        pat = build_pattern(r, poly_expr_col, poly_tokens_col, poly_lemmas_col, poly_upos_col, poly_deprel_col, poly_heads_col)\n","        if pat:\n","            patterns.append(pat)\n","\n","    print(f\"Patterns loaded: {len(patterns)}\")\n","\n","    # --- Occurrence-level collection (SAFE, no JSON-in-cell)\n","    occ_rows = []  # one row per occurrence\n","    for sent_idx, row in tqdm(corpus_df.iterrows(), total=len(corpus_df), desc=\"Projecting occurrences\"):\n","        sent_ud = parse_ud_row(row, corp_tokens_col, corp_lemmas_col, corp_upos_col, corp_deprel_col, corp_heads_col)\n","        if sent_ud[\"n\"] == 0:\n","            continue\n","\n","        sent_keys = set(zip(sent_ud[\"lemmas\"], sent_ud[\"upos\"]))\n","        sent_text = safe_text(row.get(sent_text_col, \"\"))\n","        niveau = safe_text(row.get(niveau_col, \"\")) if niveau_col else \"\"\n","        source = safe_text(row.get(source_col, \"\")) if source_col else \"\"\n","\n","        for pat in patterns:\n","            if not pat[\"keys\"].issubset(sent_keys):\n","                continue\n","            matches = find_matches(pat, sent_ud)\n","            if not matches:\n","                continue\n","\n","            for ids in matches:\n","                occ_rows.append({\n","                    \"expression\": pat[\"expression\"],\n","                    \"sent_id\": int(sent_idx),\n","                    \"sentence\": sent_text,\n","                    \"niveau\": niveau,\n","                    \"source\": source,\n","                    \"token_ids_1based\": \" \".join(map(str, ids)),\n","                })\n","\n","    occ_df = pd.DataFrame(occ_rows)\n","\n","    # --- Pivot by niveau (counts)\n","    pivot = occ_df.pivot_table(\n","        index=\"expression\",\n","        columns=\"niveau\",\n","        values=\"sent_id\",\n","        aggfunc=\"count\",\n","        fill_value=0\n","    ).reset_index()\n","\n","    # --- Wide table: expression + annotation + occurrence_count + occ_1..occ_N\n","    # Build occurrence string per row\n","    def occ_to_cell(r):\n","        # format: [niveau] [source] sentence\n","        return f\"[{r['niveau']}] [{r['source']}] {trim(r['sentence'], MAX_SENT_CHARS)}\"\n","\n","    occ_df[\"occ_cell\"] = occ_df.apply(occ_to_cell, axis=1)\n","\n","    # group occurrences\n","    grouped = occ_df.groupby(\"expression\", sort=False)[\"occ_cell\"].apply(list).reset_index()\n","    grouped[\"occurrence_count\"] = grouped[\"occ_cell\"].apply(len)\n","    grouped[\"annotation_resolved\"] = grouped[\"expression\"].map(lambda e: expr2ann.get(e, \"\"))\n","\n","    # Expand to columns occ_1..occ_N (capped)\n","    max_len = int(min(MAX_OCC_COLS, grouped[\"occ_cell\"].apply(len).max() if len(grouped) else 0))\n","\n","    wide = grouped[[\"expression\", \"annotation_resolved\", \"occurrence_count\"]].copy()\n","    for k in range(1, max_len + 1):\n","        wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","\n","    # --- Merge pivot with annotation + occurrence_count\n","    # Ensure we carry annotation & count in pivot too\n","    pivot = pivot.merge(\n","        wide[[\"expression\", \"annotation_resolved\", \"occurrence_count\"]],\n","        on=\"expression\",\n","        how=\"left\"\n","    )\n","    # reorder: expression, resolved_annotation, occurrence_count, then niveaux...\n","    niveau_cols = [c for c in pivot.columns if c not in [\"expression\", \"annotation_resolved\", \"occurrence_count\"]]\n","    pivot = pivot[[\"expression\", \"annotation_resolved\", \"occurrence_count\"] + sorted(niveau_cols)]\n","\n","    # Save\n","    wide.to_excel(OUT_WIDE_PATH, index=False)\n","    pivot.to_excel(OUT_PIVOT_PATH, index=False)\n","    print(\"Saved wide:\", OUT_WIDE_PATH)\n","    print(\"Saved pivot:\", OUT_PIVOT_PATH)\n","    print(\"Note: occ columns capped at\", MAX_OCC_COLS, \"and sentence trimmed to\", MAX_SENT_CHARS, \"chars.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUY1YhM-Be0h","executionInfo":{"status":"ok","timestamp":1770047208568,"user_tz":-180,"elapsed":47159,"user":{"displayName":"Anna Kalinina","userId":"16156166223513079181"}},"outputId":"d0b6a0e2-0eba-4f34-fd5e-2bdd2150a5f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Building patterns: 100%|██████████| 2260/2260 [00:00<00:00, 13235.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Patterns loaded: 2260\n"]},{"output_type":"stream","name":"stderr","text":["Projecting occurrences: 100%|██████████| 49263/49263 [00:22<00:00, 2190.89it/s]\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n","/tmp/ipython-input-254325365.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  wide[f\"occ_{k}\"] = grouped[\"occ_cell\"].apply(lambda lst: lst[k-1] if len(lst) >= k else \"\")\n"]},{"output_type":"stream","name":"stdout","text":["Saved wide: mwe_occurrences_wide.xlsx\n","Saved pivot: mwe_par_niveau_pivot.xlsx\n","Note: occ columns capped at 500 and sentence trimmed to 250 chars.\n"]}]}]}