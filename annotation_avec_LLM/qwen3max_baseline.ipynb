{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annakalinina18/star-fle/blob/main/annotation_with_LLM/qwen3max_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "# =========================\n",
        "# 0. CLIENT (Qwen via DashScope OpenAI-compatible API)\n",
        "# =========================\n",
        "# export DASHSCOPE_API_KEY=\"...\"\n",
        "api_key = \"\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n",
        "    # si besoin (région Chine) :\n",
        "    # base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 1. MODÈLE\n",
        "# =========================\n",
        "MODEL_NAME = \"qwen3-max\"\n",
        "\n",
        "# =========================\n",
        "# 2. RETRY / THROTTLE\n",
        "# =========================\n",
        "MAX_RETRIES = 6\n",
        "MAX_BACKOFF_SEC = 25.0\n",
        "THROTTLE_PER_CALL_SEC = 0.10\n",
        "\n",
        "def _is_retryable_error(e: Exception) -> bool:\n",
        "    msg = str(e).lower()\n",
        "    return (\n",
        "        \"429\" in msg or \"rate\" in msg or\n",
        "        \"503\" in msg or \"unavailable\" in msg or\n",
        "        \"overload\" in msg or \"timeout\" in msg or\n",
        "        \"server\" in msg\n",
        "    )\n",
        "\n",
        "def chat_completion_with_retry(messages, temperature=0, max_tokens=80):\n",
        "    last_exc = None\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "            )\n",
        "            time.sleep(THROTTLE_PER_CALL_SEC)\n",
        "            return resp\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            if _is_retryable_error(e):\n",
        "                wait = min(MAX_BACKOFF_SEC, (2 ** attempt) + random.uniform(0, 1))\n",
        "                print(f\"⚠️ API retryable error — retry in {wait:.1f}s ({attempt+1}/{MAX_RETRIES})\")\n",
        "                time.sleep(wait)\n",
        "                continue\n",
        "            raise\n",
        "    raise RuntimeError(f\"Échec après retries. Dernière erreur: {last_exc}\")\n",
        "\n",
        "# =========================\n",
        "# 3. BASELINE PROMPT (4 définitions générales)\n",
        "# =========================\n",
        "BASELINE_PROMPT = \"\"\"\n",
        "Tu es linguiste.\n",
        "\n",
        "Ta tâche est de classer une expression nominale française\n",
        "dans UNE SEULE des catégories suivantes, en te fondant sur\n",
        "une compréhension linguistique générale (pas de guide spécifique).\n",
        "\n",
        "Catégories :\n",
        "\n",
        "1) Expression_idiomatique\n",
        "Expression figée dont le sens global n’est pas directement déductible\n",
        "du sens littéral de ses mots.\n",
        "Exemples : « lune de miel », « forêt noire » (gâteau)\n",
        "\n",
        "2) Collocation_opaque\n",
        "Association de mots relativement conventionnelle,\n",
        "dont le sens implique une image, une métaphore ou une métonymie.\n",
        "Exemples : « fil rouge », « train de vie »\n",
        "\n",
        "3) Collocation_transparente\n",
        "Association de mots fréquente ou conventionnelle,\n",
        "dont le sens est globalement déductible des mots qui la composent.\n",
        "Exemples : « événement culturel », « roman policier »\n",
        "\n",
        "4) Expression_libre\n",
        "Combinaison de mots construite librement en discours,\n",
        "sans caractère figé ou conventionnel particulier.\n",
        "Exemples : « livre intéressant », « maison ancienne »\n",
        "\n",
        "Contraintes :\n",
        "- Choisis UNE seule catégorie.\n",
        "- Réponds uniquement selon le format ci-dessous.\n",
        "\n",
        "FORMAT OBLIGATOIRE :\n",
        "Catégorie : <Expression_idiomatique | Collocation_opaque | Collocation_transparente | Expression_libre>\n",
        "Explication : <une phrase très courte>\n",
        "\n",
        "Expression : {expression}\n",
        "Contexte : {contexte}\n",
        "\"\"\"\n",
        "\n",
        "ALLOWED = {\n",
        "    \"Expression_idiomatique\",\n",
        "    \"Collocation_opaque\",\n",
        "    \"Collocation_transparente\",\n",
        "    \"Expression_libre\",\n",
        "}\n",
        "\n",
        "def extract_category(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"INVALID\"\n",
        "    m = re.search(r\"(?im)^\\s*catégorie\\s*:\\s*(.+?)\\s*$\", text)\n",
        "    if not m:\n",
        "        return \"INVALID\"\n",
        "    cat = m.group(1).strip()\n",
        "    return cat if cat in ALLOWED else \"INVALID\"\n",
        "\n",
        "# =========================\n",
        "# 4. 1 appel / expression\n",
        "# =========================\n",
        "def classify_baseline(expression, examples):\n",
        "    contexte = \"\" if examples is None or pd.isna(examples) else str(examples)\n",
        "    prompt = BASELINE_PROMPT.format(\n",
        "        expression=str(expression).strip(),\n",
        "        contexte=contexte\n",
        "    )\n",
        "\n",
        "    resp = chat_completion_with_retry(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_tokens=80,\n",
        "    )\n",
        "\n",
        "    raw = (resp.choices[0].message.content or \"\").strip()\n",
        "    cat = extract_category(raw)\n",
        "    return cat, raw\n",
        "\n",
        "# =========================\n",
        "# 5. TRAITEMENT EXCEL\n",
        "# =========================\n",
        "input_file = \"nominal_part_7.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "df[\"llm_category\"] = None\n",
        "df[\"llm_raw_response\"] = None\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Baseline (Qwen3-Max)\"):\n",
        "    expr = row.get(\"expression\")\n",
        "    ex = row.get(\"examples_joined\")\n",
        "\n",
        "    if pd.isna(expr) or not str(expr).strip():\n",
        "        df.at[idx, \"llm_category\"] = \"N/A\"\n",
        "        df.at[idx, \"llm_raw_response\"] = \"N/A\"\n",
        "        continue\n",
        "\n",
        "    cat, raw = classify_baseline(expr, ex)\n",
        "    df.at[idx, \"llm_category\"] = cat\n",
        "    df.at[idx, \"llm_raw_response\"] = raw\n",
        "\n",
        "output_file = \"annotated_nominal_part_7_qwen3_max_baseline_4defs.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Saved: {output_file}\")\n"
      ],
      "metadata": {
        "id": "7BxZab6HbaMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8468947a-30e0-4a66-a322-42fb019e08ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline (Qwen3-Max): 100%|██████████| 100/100 [03:54<00:00,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: annotated_nominal_part_7_qwen3_max_baseline_4defs.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
